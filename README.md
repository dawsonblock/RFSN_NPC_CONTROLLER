# ğŸ® RFSN Orchestrator

<div align="center">

**Production-Ready Streaming AI System for Real-Time NPC Dialogue**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-009688.svg)](https://fastapi.tiangolo.com/)
[![Tests](https://img.shields.io/badge/tests-142%20passing-success.svg)](Python/tests/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/code%20style-optimized-brightgreen.svg)](Python/)

*Production-ready streaming AI with semantic action selection, world model prediction, and real-time TTS*

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Architecture](#-architecture) â€¢ [API](#-api-reference) â€¢ [Performance](#-performance)

</div>

---

## ğŸŒŸ Features

### Core Capabilities

- **ğŸ§  Intelligent Tokenization** - Smart sentence detection with abbreviation handling (Dr., Mr., Jarl)
- **ğŸ¯ Semantic Action Selection** - World model predicts outcomes and scores NPC actions (GREET, APOLOGIZE, THREATEN, etc.)
- **ğŸ™ï¸ Real-Time TTS** - Kokoro-ONNX engine with streaming audio playback
- **âš¡ Thread-Safe Queue** - Deque+Condition pattern eliminates race conditions
- **ğŸ”’ Atomic Runtime** - Safe hot-reloads without half-applied config
- **ğŸ“Š Live Metrics** - WebSocket-based performance monitoring dashboard
- **ğŸ’¾ Persistent Memory** - Conversation history with automatic backups
- **ğŸ¤– Adaptive Learning** - Contextual bandit learns optimal dialogue styles per NPC
- **ğŸ›¡ï¸ Safety Rules** - Hard overrides prevent learned stupidity in combat/trust/quest contexts

### Production Hardening (v9.0)

- âœ… **142 Tests** - Comprehensive coverage including edge cases, learning layer, and world model integration
- âœ… **Zero Race Conditions** - Deque+Condition queue pattern (no task_done/join bugs)
- âœ… **Atomic State Swaps** - RuntimeState prevents half-applied config during reloads
- âœ… **Single TTS Queue** - Unified backpressure (no double-buffering)
- âœ… **Canonical Versioning** - Single source of truth for version strings
- âœ… **Safety Rules** - Hard overrides prevent learned stupidity in critical states

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+ (required for Kokoro TTS)
- 4GB RAM minimum
- macOS, Linux, or Windows
- Ollama (for local LLM)

### Installation

```bash
# Clone the repository
git clone https://github.com/dawsonblock/RFSN-ORCHESTRATOR.git
cd RFSN-ORCHESTRATOR

# Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r Python/requirements-core.txt
pip install kokoro-onnx

# Install Ollama (macOS)
brew install ollama
ollama serve &
ollama pull llama3.2

# Launch the server
cd Python
python -m uvicorn orchestrator:app --host 0.0.0.0 --port 8000
```

**Server URL**: `http://127.0.0.1:8000`  
**Dashboard**: `http://127.0.0.1:8000`
**Mobile UI**: `http://127.0.0.1:8080` (run `python mobile_chat/server.py`)

### Docker (Optional)

```bash
docker build -t rfsn-orchestrator .
docker run -p 8000:8000 rfsn-orchestrator
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RFSN Orchestrator v9.0                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   FastAPI    â”‚â”€â”€â”€â–¶â”‚  Streaming   â”‚â”€â”€â”€â–¶â”‚ Kokoro TTS   â”‚  â”‚
â”‚  â”‚   Server     â”‚    â”‚   Engine     â”‚    â”‚ (ONNX/async) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â–¼                    â–¼                    â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Learning   â”‚    â”‚ DequeSpeech  â”‚    â”‚   Audio      â”‚  â”‚
â”‚  â”‚    Layer     â”‚    â”‚    Queue     â”‚    â”‚   Player     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                    â”‚                               â”‚
â”‚         â–¼                    â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚  World Model â”‚â”€â”€â”€â–¶â”‚Action Scorer â”‚                       â”‚
â”‚  â”‚  (Prediction)â”‚    â”‚  (Scoring)   â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | Purpose | Location |
|-----------|---------|----------|
| **Orchestrator** | FastAPI server, request handling | `Python/orchestrator.py` |
| **Streaming Engine** | Token processing, sentence detection | `Python/streaming_engine.py` |
| **DequeSpeechQueue** | Thread-safe bounded queue with drop policy | `Python/streaming_voice_system.py` |
| **World Model** | Predicts state transitions from actions | `Python/world_model.py` |
| **Action Scorer** | Scores candidate actions using predictions | `Python/action_scorer.py` |
| **Learning Layer** | Contextual bandit for dialogue style selection | `Python/learning/` |
| **Runtime State** | Atomic state management for safe reloads | `Python/runtime_state.py` |
| **State Machine** | Authoritative state transitions with invariants | `Python/state_machine.py` |
| **Memory Manager** | Conversation persistence, backups | `Python/memory_manager.py` |
| **Kokoro TTS** | Text-to-speech synthesis (ONNX) | `Python/kokoro_tts.py` |
| **Ollama Client** | Local LLM HTTP API client | `Python/ollama_client.py` |
| **Mobile Chat** | iPhone-optimized chat UI | `mobile_chat/` |
| **Dashboard** | Live metrics visualization | `Dashboard/index.html` |

---

## ğŸ“¡ API Reference

### Streaming Dialogue

```http
POST /api/dialogue/stream
Content-Type: application/json

{
  "npc_name": "Jarl Balgruuf",
  "user_input": "Tell me about Whiterun."
}
```

**Response**: Server-Sent Events (SSE) stream

```
data: {"sentence": "Whiterun is a great city.", "is_final": false, "latency_ms": 150}
data: {"sentence": "We welcome all travelers.", "is_final": true, "latency_ms": 280}
```

### Memory Management

```http
# Get memory stats
GET /api/memory/{npc_name}/stats

# Safe reset with backup
POST /api/memory/{npc_name}/safe_reset

# List backups
GET /api/memory/{npc_name}/backups
```

### Performance Tuning

```http
POST /api/tune-performance
Content-Type: application/json

{
  "temperature": 0.7,
  "max_tokens": 150,
  "max_queue_size": 3
}
```

### Health & Metrics

```http
# Health check
GET /api/health

# WebSocket metrics stream
WS /ws/metrics
```

---

## âš¡ Performance

### Benchmarks

| Metric | Target | Actual |
|--------|--------|--------|
| First Token Latency | <1.5s | ~1.2s |
| Sentence Detection | <50ms | ~30ms |
| TTS Processing | <100ms | ~80ms |
| Queue Throughput | 10 items/s | 12 items/s |

### Key Optimizations

- **Deque+Condition Queue** - Eliminates task_done/join race conditions
- **Atomic Drop Policy** - Drop runs under same lock as worker get()
- **Pre-compiled Regex** - Eliminates hot-path compilation overhead
- **Sync-Only TTS** - Single queue path prevents double-buffering
- **Config Snapshots** - Per-request snapshots prevent mid-stream changes

---

## ğŸ§ª Testing

### Run All Tests

```bash
cd Python
python -m pytest tests/ -v
```

### Test Coverage

- **Core Functionality**: 105 tests
- **Learning Layer**: 21 tests
- **World Model Integration**: 3 tests
- **Edge Cases**: 13 tests
- **Total**: 142 tests (100% passing)

### Test Categories

```bash
# Unit tests
pytest tests/test_streaming_fixes.py -v

# Learning layer tests
pytest tests/test_learning.py tests/test_learning_policy.py -v

# Performance tests
pytest tests/test_performance.py -v

# Edge cases
pytest tests/test_edge_cases.py -v

# Backpressure tests
pytest tests/test_backpressure.py -v
```

---

## ğŸ¯ World Model & Action Scoring

The system includes a world model that predicts state transitions and scores NPC actions based on predicted outcomes:

### NPC Actions

| Action | Description | Use Case |
|--------|-------------|----------|
| `GREET` | Warmly welcome the player | First meeting, friendly encounters |
| `APOLOGIZE` | Express regret for mistakes | After conflicts, mistakes |
| `THREATEN` | Intimidate the player | Combat, hostile situations |
| `COMPLIMENT` | Praise the player | Building rapport |
| `HELP` | Offer assistance | Quests, requests for aid |
| `REQUEST` | Ask for something | Quest objectives |
| `AGREE` | Accept player proposals | Cooperation |
| `DISAGREE` | Reject player proposals | Conflict, boundaries |
| `IGNORE` | Dismiss the player | Low trust, busy |
| `INQUIRE` | Ask questions | Information gathering |
| `EXPLAIN` | Provide information | Lore, instructions |

### Player Signals

The system classifies player input into discrete signals:

- `GREET` - Friendly greetings
- `INSULT` - Hostile language
- `HELP` - Requests for assistance
- `THREATEN` - Combat initiation
- `APOLOGIZE` - Conciliatory language
- `QUESTION` - Information seeking
- `REQUEST` - Demands or asks

### Safety Rules

Hard overrides prevent learned stupidity in critical states:

- **Combat + Fear > 0.7** â†’ Forces `FLEE` action
- **Trust < 0.1** â†’ Forbids `ACCEPT`, `OFFER`, `HELP` actions
- **Quest Active** â†’ Biases toward `HELP`, `ACCEPT`, `AGREE` actions

### How It Works

1. **Player Signal Classification** - Regex-based pattern matching with word boundaries
2. **State Snapshot Creation** - Captures current NPC state (mood, affinity, trust, fear)
3. **Candidate Proposal** - Generates 5-9 candidate actions with safety overrides
4. **World Model Prediction** - Predicts outcome state for each action
5. **Utility Scoring** - Scores predicted states using affinity, trust, fear weights
6. **Action Selection** - Selects highest-scoring action
7. **Prompt Injection** - Injects action instruction into LLM system prompt
8. **State Transition** - Applies authoritative state update via `apply_transition()`

---

## ğŸ¤– Learning Layer

The system includes a lightweight contextual bandit that learns optimal dialogue styles per NPC:

### Action Modes

| Mode | Description |
|------|-------------|
| `TERSE_DIRECT` | Short, factual responses (3-4 sentences) |
| `WARM_FRIENDLY` | Empathetic, relational responses |
| `LORE_RICH` | Detailed world-building responses |
| `PLAYFUL_WITTY` | Humorous, light-hearted responses |
| `FORMAL_RESPECTFUL` | Distant, proper responses |
| `NEUTRAL_BALANCED` | Default balanced approach |

### How It Works

1. **Feature Extraction** - Extracts 10 features from NPC state, conversation history, and memory retrieval
2. **Policy Selection** - Îµ-greedy exploration over action modes with linear weights
3. **Reward Learning** - Updates policy based on conversation signals (continuation, correction, follow-up)

### Safety Guarantees

- Learning is scoped to style selection only (not model weights)
- Weights are bounded with decay and clipping
- Atomic weight persistence
- Per-NPC isolation

---

## ğŸ¯ Running Episode Simulations

### How to Run One NPC Through 100 Turns and Export Episode Data

For testing, evaluation, or machine learning experiments, you can run extended NPC conversations and export the complete episode data.

#### 1. Create an Episode Runner Script

Create `Python/run_episode.py`:

```python
"""
Run a multi-turn NPC episode and export structured logs.
"""
import asyncio
import json
import time
from pathlib import Path
from datetime import datetime

from orchestrator import app
from world_model import StateSnapshot, PlayerSignal, NPCAction
from action_scorer import ActionScorer
from learning.bandit_core import BanditCore


async def run_episode(npc_id: str, num_turns: int = 100, output_file: str = None):
    """
    Run a simulated NPC episode with predefined player inputs.
    
    Args:
        npc_id: Unique identifier for the NPC
        num_turns: Number of conversation turns to simulate
        output_file: Path to save episode.jsonl (defaults to data/episodes/{npc_id}_{timestamp}.jsonl)
    """
    # Initialize components
    action_scorer = ActionScorer()
    bandit = BanditCore(strategy="thompson_beta", seed=42)
    
    # Initialize NPC state
    state = StateSnapshot(
        mood="neutral",
        affinity=0.0,
        relationship="stranger",
        recent_sentiment=0.0,
        combat_active=False,
        quest_active=False,
        trust_level=0.5,
        fear_level=0.0
    )
    
    # Predefined player signals for simulation
    player_signals = [
        PlayerSignal.GREET,
        PlayerSignal.QUESTION,
        PlayerSignal.COMPLIMENT,
        PlayerSignal.REQUEST,
        PlayerSignal.AGREE,
        PlayerSignal.DISAGREE,
        PlayerSignal.INSULT,
        PlayerSignal.APOLOGIZE,
        PlayerSignal.THREATEN,
        PlayerSignal.FAREWELL,
    ]
    
    # Episode log
    episode_data = []
    
    print(f"Starting episode: {npc_id} for {num_turns} turns")
    print("=" * 60)
    
    for turn in range(num_turns):
        # Select player signal (cycle through predefined signals)
        player_signal = player_signals[turn % len(player_signals)]
        
        print(f"\nTurn {turn + 1}/{num_turns}")
        print(f"Player Signal: {player_signal.value}")
        print(f"State: mood={state.mood}, affinity={state.affinity:.2f}, rel={state.relationship}")
        
        # Generate and score candidates
        candidates = action_scorer.propose_candidates(state, player_signal)
        scored_candidates = [
            (action, action_scorer.score_action(state, action, player_signal))
            for action in candidates
        ]
        
        # Select top-K
        top_k = sorted(scored_candidates, key=lambda x: x[1].total_score, reverse=True)[:4]
        top_actions = [action for action, _ in top_k]
        
        # Bandit selection
        bandit_key = f"{state.mood}|{state.relationship}|{player_signal.value}"
        priors = {action: score.total_score for action, score in top_k}
        selected_action = bandit.select(bandit_key, [a.value for a in top_actions], priors)
        selected_action_enum = NPCAction(selected_action)
        
        # Simulate reward (in real system, comes from player engagement)
        # For simulation, use scorer confidence as proxy
        selected_score = next(score for action, score in top_k if action == selected_action_enum)
        reward = min(1.0, max(0.0, (selected_score.total_score / 10.0)))
        
        # Update bandit
        bandit.update(bandit_key, selected_action, reward)
        
        # Log turn data
        turn_data = {
            "turn": turn + 1,
            "timestamp": time.time(),
            "player_signal": player_signal.value,
            "state_before": state.to_dict(),
            "candidates": [action.value for action in top_actions],
            "scores": {action.value: score.total_score for action, score in top_k},
            "bandit_key": bandit_key,
            "selected_action": selected_action,
            "reward": reward,
            "state_after": None  # Will update after transition
        }
        
        # Apply state transition (simplified)
        # In real system, use state_machine.apply_transition()
        if selected_action_enum == NPCAction.COMPLIMENT or player_signal == PlayerSignal.COMPLIMENT:
            state.affinity = min(1.0, state.affinity + 0.05)
        elif selected_action_enum == NPCAction.INSULT or player_signal == PlayerSignal.INSULT:
            state.affinity = max(-1.0, state.affinity - 0.1)
        elif selected_action_enum == NPCAction.THREATEN or player_signal == PlayerSignal.THREATEN:
            state.fear_level = min(1.0, state.fear_level + 0.15)
        
        # Update relationship tier based on affinity
        if state.affinity < -0.6:
            state.relationship = "enemy"
        elif state.affinity < -0.2:
            state.relationship = "rival"
        elif state.affinity < 0.2:
            state.relationship = "acquaintance"
        elif state.affinity < 0.6:
            state.relationship = "friend"
        else:
            state.relationship = "ally"
        
        turn_data["state_after"] = state.to_dict()
        episode_data.append(turn_data)
        
        print(f"Selected Action: {selected_action}")
        print(f"Reward: {reward:.2f}")
    
    # Save episode data
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"data/episodes/{npc_id}_{timestamp}.jsonl"
    
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w") as f:
        for turn_data in episode_data:
            f.write(json.dumps(turn_data) + "\n")
    
    print("\n" + "=" * 60)
    print(f"Episode complete! Saved to: {output_path}")
    print(f"Final state: affinity={state.affinity:.2f}, relationship={state.relationship}")
    
    # Print summary statistics
    total_reward = sum(t["reward"] for t in episode_data)
    avg_reward = total_reward / len(episode_data)
    print(f"Average reward: {avg_reward:.3f}")
    
    return episode_data


if __name__ == "__main__":
    asyncio.run(run_episode("lydia_001", num_turns=100))
```

#### 2. Run the Episode

```bash
cd Python
python run_episode.py
```

#### 3. Analyze Episode Data

The episode data is saved in JSONL format (one JSON object per line). Each line contains:

```json
{
  "turn": 1,
  "timestamp": 1705456789.123,
  "player_signal": "greet",
  "state_before": {"mood": "neutral", "affinity": 0.0, ...},
  "candidates": ["greet", "inquire", "ignore", "help"],
  "scores": {"greet": 8.5, "inquire": 6.2, ...},
  "bandit_key": "neutral|stranger|greet",
  "selected_action": "greet",
  "reward": 0.85,
  "state_after": {"mood": "friendly", "affinity": 0.05, ...}
}
```

#### 4. Visualize Episode Results

Create `Python/analyze_episode.py`:

```python
"""
Analyze and visualize episode data.
"""
import json
import matplotlib.pyplot as plt
from pathlib import Path

def analyze_episode(episode_file: str):
    """Load and analyze episode data."""
    with open(episode_file, "r") as f:
        episode = [json.loads(line) for line in f]
    
    turns = [t["turn"] for t in episode]
    rewards = [t["reward"] for t in episode]
    affinities = [t["state_after"]["affinity"] for t in episode]
    
    # Plot rewards over time
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(turns, rewards)
    plt.xlabel("Turn")
    plt.ylabel("Reward")
    plt.title("Reward Over Time")
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(turns, affinities)
    plt.xlabel("Turn")
    plt.ylabel("Affinity")
    plt.title("NPC Affinity Over Time")
    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(episode_file.replace(".jsonl", "_analysis.png"))
    print(f"Saved analysis plot: {episode_file.replace('.jsonl', '_analysis.png')}")
    
    # Print action distribution
    actions = [t["selected_action"] for t in episode]
    action_counts = {}
    for action in actions:
        action_counts[action] = action_counts.get(action, 0) + 1
    
    print("\nAction Distribution:")
    for action, count in sorted(action_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"  {action}: {count} ({count/len(episode)*100:.1f}%)")

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        analyze_episode(sys.argv[1])
    else:
        print("Usage: python analyze_episode.py <episode.jsonl>")
```

Run analysis:

```bash
python analyze_episode.py data/episodes/lydia_001_20240116_123456.jsonl
```

#### 5. Integration Testing with Real Orchestrator

For full end-to-end testing with the live orchestrator:

```bash
# Start orchestrator
python launch_optimized.py

# In another terminal, run episode via HTTP API
python -c "
import requests
import json

for i in range(100):
    response = requests.post(
        'http://localhost:8000/api/chat',
        json={
            'message': f'Turn {i+1} test message',
            'npc_id': 'test_npc',
            'context': {}
        }
    )
    print(f'Turn {i+1}: {response.status_code}')
"
```

---

## ğŸ“Š Monitoring

### Dashboard Features

- **Real-time Metrics** - First token, sentence, and total generation latency
- **Queue Status** - Current size and dropped sentence count
- **Performance Tuning** - Live adjustment of temperature, tokens, queue size
- **Visual Feedback** - Glassmorphism UI with smooth animations

### Metrics Available

```javascript
{
  "first_token_ms": 1200,
  "first_sentence_ms": 1500,
  "total_generation_ms": 3000,
  "tts_queue_size": 2,
  "dropped_sentences": 0
}
```

---

## ğŸ”§ Configuration

### `config.json`

```json
{
  "llm": {
    "backend": "ollama",
    "ollama_host": "http://localhost:11434",
    "ollama_model": "llama3.2",
    "llm_model_path": "Models/Mantella-Skyrim-Llama-3-8B-Q4_K_M.gguf"
  },
  "tts": {
    "backend": "kokoro",
    "voice": "af_bella",
    "speed": 1.0
  },
  "temperature": 0.7,
  "max_tokens": 150,
  "max_queue_size": 3,
  "log_level": "INFO"
}
```

### Environment Variables

```bash
export RFSN_PORT=8000
export RFSN_HOST=0.0.0.0
export RFSN_LOG_LEVEL=DEBUG
```

---

## ğŸ› ï¸ Development

### Project Structure

```
RFSN-ORCHESTRATOR/
â”œâ”€â”€ Python/
â”‚   â”œâ”€â”€ orchestrator.py           # FastAPI server
â”‚   â”œâ”€â”€ streaming_engine.py       # Core streaming logic
â”‚   â”œâ”€â”€ streaming_voice_system.py # DequeSpeechQueue (thread-safe)
â”‚   â”œâ”€â”€ runtime_state.py          # Atomic runtime management
â”‚   â”œâ”€â”€ version.py                # Canonical version strings
â”‚   â”œâ”€â”€ memory_manager.py         # Conversation persistence
â”‚   â”œâ”€â”€ piper_tts.py              # TTS engine
â”‚   â”œâ”€â”€ security.py               # Authentication
â”‚   â”œâ”€â”€ learning/                 # Contextual bandit layer
â”‚   â”‚   â”œâ”€â”€ schemas.py            # ActionMode definitions
â”‚   â”‚   â”œâ”€â”€ policy_adapter.py     # Feature extraction + action selection
â”‚   â”‚   â”œâ”€â”€ reward_model.py       # Reward computation
â”‚   â”‚   â””â”€â”€ trainer.py            # Online weight updates
â”‚   â”œâ”€â”€ requirements.txt          # Dependencies
â”‚   â””â”€â”€ tests/                    # Test suite (139 tests)
â”œâ”€â”€ Dashboard/
â”‚   â””â”€â”€ index.html                # Metrics dashboard
â”œâ”€â”€ Models/
â”‚   â””â”€â”€ piper/                    # Voice models
â”œâ”€â”€ config.json                   # Configuration
â””â”€â”€ README.md                     # This file
```

### Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Run tests (`pytest tests/ -v`)
4. Commit changes (`git commit -m 'Add amazing feature'`)
5. Push to branch (`git push origin feature/amazing-feature`)
6. Open a Pull Request

---

## ğŸ“ˆ Changelog

### v9.0 (Latest) - Thread-Safe Queue Rewrite

- **Deque+Condition queue** replaces queue.Queue (eliminates race conditions)
- Removed all task_done/join semantics
- Drop policy runs atomically with consumer
- Added `RuntimeState` for atomic config swaps
- Added `version.py` for canonical versioning
- Removed dead code (StreamingVoiceSystemV2)
- Added `enable_queue` flag to Piper TTS

### v8.10 - Critical Bug Fixes

- Fixed `/ws/metrics` crash (missing `asdict` import)
- Corrected end-of-stream flush semantics
- Renamed `flush()` â†’ `reset()`, added `flush_pending()`

### v8.9 - Operational Hardening

- Tokenizer continuation fix
- Explicit end-of-stream flush
- Stream hygiene improvements
- `/api/health` endpoint
- Structured trace logging

### v8.8 - Backpressure & Resize Hardening

- Coherent queue trimming (keep first/last)
- Worker wakeup sentinel
- Thread-safe resize operations

### v8.7 - Reliability Hardening

- Smart sentence boundary detection
- Quote-aware tokenization
- Newline boundary support

[View Full Changelog](CHANGELOG.md)

---

## ğŸ¤ Acknowledgments

- **LLaMA** - Meta's language model
- **Piper TTS** - Rhasspy's neural TTS engine
- **FastAPI** - Modern Python web framework

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details

---

## ğŸ”— Links

- **Repository**: [github.com/dawsonblock/RFSN-ORCHESTRATOR](https://github.com/dawsonblock/RFSN-ORCHESTRATOR)
- **Issues**: [Report a bug](https://github.com/dawsonblock/RFSN-ORCHESTRATOR/issues)
- **Discussions**: [Join the conversation](https://github.com/dawsonblock/RFSN-ORCHESTRATOR/discussions)

---

<div align="center">

**Made with â¤ï¸ for immersive NPC interactions**

â­ Star this repo if you find it useful!

</div>
